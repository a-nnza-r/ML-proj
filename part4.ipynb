{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "--- PREDICTING ES ---\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nryan\\AppData\\Local\\Temp\\ipykernel_4196\\240872279.py:133: RuntimeWarning: invalid value encountered in divide\n",
      "  transmission_probabilities = transition_counts / state_counts[:, None]\n",
      "C:\\Users\\nryan\\AppData\\Local\\Temp\\ipykernel_4196\\240872279.py:185: RuntimeWarning: invalid value encountered in divide\n",
      "  emission_probabilities = (np.maximum(emission_counts - d, 0)) / state_counts[:, None] # normalise by state counts\n",
      "C:\\Users\\nryan\\AppData\\Local\\Temp\\ipykernel_4196\\240872279.py:186: RuntimeWarning: invalid value encountered in divide\n",
      "  unk_prob = d * np.count_nonzero(emission_counts, axis=1) / state_counts # Multiply discount d by the number of non-zero emission counts and divide by state counts\n",
      "C:\\Users\\nryan\\AppData\\Local\\Temp\\ipykernel_4196\\240872279.py:158: RuntimeWarning: invalid value encountered in divide\n",
      "  lambdas = unique_observations_per_state / (unique_observations_per_state + state_counts) # T / (T + N) where T is the number of unique observations and N is the number of observations of that state\n",
      "C:\\Users\\nryan\\AppData\\Local\\Temp\\ipykernel_4196\\240872279.py:160: RuntimeWarning: invalid value encountered in divide\n",
      "  emission_probabilities = lambdas[:, None] * (emission_counts / state_counts[:, None]) + (1 - lambdas[:, None]) / num_observations #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "--- PREDICTING RU ---\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evalResult\n",
    "\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    \"\"\"Prepare the raw data for training\n",
    "\n",
    "    Args:\n",
    "        file_path (string): input file path for preparation\n",
    "\n",
    "    Returns:\n",
    "        dictionary: state_to_idx - a mapping of each state to the index\n",
    "        dictionary: observation_to_idx - a mapping of each observation to the index\n",
    "        list: states - a list of unique states\n",
    "        list: observations - a list of unique observations\n",
    "        string: train_data - training data as a whole string\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    states = set()\n",
    "    observations = set()\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()  # Remove trailing newline characters\n",
    "        if line:  # Non-empty line\n",
    "            space_idx = line.rfind(\" \")\n",
    "            observation = line[:space_idx]\n",
    "            state = line[space_idx + 1:]\n",
    "            states.add(state)\n",
    "            observations.add(observation)\n",
    "            data.append((observation, state))\n",
    "\n",
    "    # add START and STOP states\n",
    "    states.add(\"START\")\n",
    "    states.add(\"STOP\")\n",
    "\n",
    "    # add #UNK# observation\n",
    "    observations.add(\"#UNK#\")\n",
    "\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(sorted(states))}\n",
    "    observation_to_idx = {obs: idx for idx,\n",
    "                          obs in enumerate(sorted(observations))}\n",
    "\n",
    "    # read train_data\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        train_data = file.read()\n",
    "\n",
    "    return state_to_idx, observation_to_idx, sorted(states), sorted(observations), train_data\n",
    "\n",
    "\n",
    "def estimate_emission_parameters(train_data, states, observations, state_to_idx, observation_to_idx, k=1):\n",
    "    \"\"\"Estimate the emission probabilities\n",
    "\n",
    "    Args:\n",
    "        train_data (string): Training data as a whole string\n",
    "        states (list): List of unique states\n",
    "        observations (list): List of unique observations\n",
    "        state_to_idx (dict): Mapping of each state to the index\n",
    "        observation_to_idx (dict): Mapping of each observation to the index\n",
    "        k (int): Smoothing parameter\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Emission probabilities of shape (num_states, num_observations)\n",
    "\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_observations = len(observations)\n",
    "\n",
    "    # Initialize counts\n",
    "    # rows are states, columns are observations\n",
    "    emission_counts = np.zeros((num_states, num_observations))\n",
    "    state_counts = np.zeros(num_states)\n",
    "\n",
    "    # Split the training data into sentences\n",
    "    sentences = train_data.strip().split('\\n\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            observation, state = line.rsplit(' ', 1)\n",
    "            state_idx = state_to_idx[state]\n",
    "            observation_idx = observation_to_idx[observation]\n",
    "            emission_counts[state_idx, observation_idx] += 1\n",
    "            state_counts[state_idx] += 1\n",
    "\n",
    "    # Calculate probabilities for known words\n",
    "    emission_probabilities = (\n",
    "        emission_counts + k) / (state_counts[:, None] + k * num_observations)  # LAPLACE SMOOTHING\n",
    "\n",
    "    # Calculate probabilities for unknown words\n",
    "    unk_idx = observation_to_idx[\"#UNK#\"]\n",
    "    for i in range(num_states):\n",
    "        emission_probabilities[i, unk_idx] = k / (state_counts[i] + k)\n",
    "\n",
    "    # Set emission probabilities for START and STOP states to 0\n",
    "    emission_probabilities[state_to_idx[\"START\"], :] = 0\n",
    "    emission_probabilities[state_to_idx[\"STOP\"], :] = 0\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def estimate_transmission_parameters(train_data, states, state_to_idx):\n",
    "    \"\"\"Estimate the transmission probabilities\n",
    "\n",
    "    Args:\n",
    "        train_data (string): Training data as a whole string\n",
    "        states (list): List of unique states\n",
    "        state_to_idx (dict): Mapping of each state to the index\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Transmission probabilities of shape (num_states, num_states)\n",
    "\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "\n",
    "    # Initialize counts\n",
    "    transition_counts = np.zeros((num_states, num_states))\n",
    "    state_counts = np.zeros(num_states)\n",
    "\n",
    "    # Split the training data into sentences\n",
    "    sentences = train_data.strip().split('\\n\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # separate each line as an element without the \\n\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        prev_state = \"START\"\n",
    "        for line in lines:\n",
    "            _, current_state = line.rsplit(' ', 1)\n",
    "            transition_counts[state_to_idx[prev_state],\n",
    "                              state_to_idx[current_state]] += 1\n",
    "            state_counts[state_to_idx[prev_state]] += 1\n",
    "            prev_state = current_state\n",
    "        # Transition to STOP state\n",
    "        transition_counts[state_to_idx[prev_state], state_to_idx[\"STOP\"]] += 1\n",
    "        state_counts[state_to_idx[prev_state]] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    transmission_probabilities = transition_counts / state_counts[:, None]\n",
    "\n",
    "    nan_mask = np.isnan(transmission_probabilities)\n",
    "    # set all transition probabilities from STOP to 0\n",
    "    transmission_probabilities[nan_mask] = 0\n",
    "    return transmission_probabilities\n",
    "\n",
    "# Witten-Bell smoothing\n",
    "\n",
    "\n",
    "def estimate_emission_parameters_witten_bell(train_data, states, observations, state_to_idx, observation_to_idx):\n",
    "    num_states = len(states)\n",
    "    num_observations = len(observations)\n",
    "    emission_counts = np.zeros((num_states, num_observations))\n",
    "    state_counts = np.zeros(num_states)\n",
    "    sentences = train_data.strip().split('\\n\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            observation, state = line.rsplit(' ', 1)\n",
    "            state_idx = state_to_idx[state]\n",
    "            observation_idx = observation_to_idx[observation]\n",
    "            emission_counts[state_idx, observation_idx] += 1\n",
    "            state_counts[state_idx] += 1\n",
    "\n",
    "    ### Witten-Bell smoothing ###\n",
    "    unique_observations_per_state = np.count_nonzero(emission_counts, axis=1)\n",
    "    # T / (T + N) where T is the number of unique observations and N is the number of observations of that state\n",
    "    lambdas = unique_observations_per_state / \\\n",
    "        (unique_observations_per_state + state_counts)\n",
    "\n",
    "    emission_probabilities = lambdas[:, None] * (\n",
    "        emission_counts / state_counts[:, None]) + (1 - lambdas[:, None]) / num_observations\n",
    "\n",
    "    emission_probabilities[state_to_idx[\"START\"], :] = 0\n",
    "    emission_probabilities[state_to_idx[\"STOP\"], :] = 0\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "# Absolute discounting\n",
    "\n",
    "\n",
    "def estimate_emission_parameters_absolute_discounting(train_data, states, observations, state_to_idx, observation_to_idx, d=0.5):\n",
    "    num_states = len(states)\n",
    "    num_observations = len(observations)\n",
    "    emission_counts = np.zeros((num_states, num_observations))\n",
    "    state_counts = np.zeros(num_states)\n",
    "    sentences = train_data.strip().split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            observation, state = line.rsplit(' ', 1)\n",
    "            state_idx = state_to_idx[state]\n",
    "            observation_idx = observation_to_idx[observation]\n",
    "            emission_counts[state_idx, observation_idx] += 1\n",
    "            state_counts[state_idx] += 1\n",
    "    # Calculate probabilities with absolute discounting\n",
    "\n",
    "    # the main idea here is to redistribute the weightage of words we observed to words we didn't see before #UNK#\n",
    "    emission_probabilities = (np.maximum(\n",
    "        emission_counts - d, 0)) / state_counts[:, None]  # normalise by state counts\n",
    "    # Multiply discount d by the number of non-zero emission counts and divide by state counts\n",
    "    unk_prob = d * np.count_nonzero(emission_counts, axis=1) / state_counts\n",
    "\n",
    "    emission_probabilities += unk_prob[:, None] / num_observations\n",
    "    emission_probabilities[state_to_idx[\"START\"], :] = 0\n",
    "    emission_probabilities[state_to_idx[\"STOP\"], :] = 0\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def viterbi(test_data, states, state_to_idx, observation_to_idx, emission_probabilities, transmission_probabilities):\n",
    "    sentences = test_data.strip().split('\\n\\n')\n",
    "    predicted_tags = []\n",
    "    for sentence in sentences:  # assume each chunk separated by a newline in the test data is a sentence\n",
    "        words = sentence.strip().split('\\n')\n",
    "        num_words = len(words)\n",
    "        num_states = len(states)\n",
    "\n",
    "        # initialize viterbi matrix (tabulation table) and backpointers matrix (for convenience to backtrack later)\n",
    "        viterbi_matrix = np.zeros((num_states, num_words+2))\n",
    "        # Backpointers matrix for recording the index of best previous state\n",
    "        backpointers = np.zeros((num_states, num_words+2), dtype=int)\n",
    "\n",
    "        # step 1 - initialization\n",
    "        start_idx = state_to_idx[\"START\"]\n",
    "        stop_idx = state_to_idx[\"STOP\"]\n",
    "        viterbi_matrix[start_idx, 0] = 1\n",
    "\n",
    "        # step 2 - recurrence\n",
    "        for j in range(1, num_words+1):\n",
    "            word = words[j-1]\n",
    "            if word not in observation_to_idx:\n",
    "                word = \"#UNK#\"\n",
    "            observation_idx = observation_to_idx[word]\n",
    "            for s in range(num_states):\n",
    "                if s == start_idx or s == stop_idx:\n",
    "                    continue\n",
    "                max_trans_prob = 0\n",
    "                # finding the max transition probability from all previous states to current state\n",
    "                for prev_s in range(num_states):\n",
    "                    trans_prob = transmission_probabilities[prev_s,\n",
    "                                                            s] * viterbi_matrix[prev_s, j-1]\n",
    "                    if trans_prob > max_trans_prob:\n",
    "                        max_trans_prob = trans_prob\n",
    "\n",
    "                # update viterbi matrix and backpointers matrix\n",
    "                viterbi_matrix[s, j] = max_trans_prob * \\\n",
    "                    emission_probabilities[s, observation_idx]\n",
    "                backpointers[s, j] = np.argmax(\n",
    "                    transmission_probabilities[:, s] * viterbi_matrix[:, j-1])  # Added backpointer\n",
    "\n",
    "        # step 3 - termination\n",
    "        stop_trans_prob = transmission_probabilities[:,\n",
    "                                                     stop_idx] * viterbi_matrix[:, num_words]\n",
    "        max_stop_trans_prob = np.max(stop_trans_prob)\n",
    "        viterbi_matrix[stop_idx, num_words+1] = max_stop_trans_prob\n",
    "\n",
    "        best_last_tag = np.argmax(viterbi_matrix[:, -2])\n",
    "        best_path = [best_last_tag]\n",
    "\n",
    "        # Backtrack using backpointers\n",
    "        for i in range(num_words, 1, -1):\n",
    "            best_tag = backpointers[best_last_tag, i]\n",
    "            best_path.insert(0, best_tag)\n",
    "            best_last_tag = best_tag\n",
    "\n",
    "        predicted_tags.append([states[s] for s in best_path])\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "def write_predictions_to_file(file_path, predicted_sequences, test_data):\n",
    "    \"\"\"write to file the predicted sequences\n",
    "\n",
    "    Args:\n",
    "        file_path (string): the file path and name to write to\n",
    "        predicted_sequences (a list of sentences): each sentence is a list of predicted tags\n",
    "        test_data (string): a string of the test data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "        sentences = test_data.strip().split('\\n\\n')\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            words = sentence.strip().split('\\n')\n",
    "            for j, word in enumerate(words):\n",
    "                file.write(word + ' ' + predicted_sequences[i][j] + '\\n')\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "def compute_scores(gold_file_path, predicted_file_path):\n",
    "    ## code adapted from evalResult ##\n",
    "    gold = open(gold_file_path, \"r\", encoding=\"utf-8\")\n",
    "    predicted = open(predicted_file_path, \"r\", encoding=\"utf-8\")\n",
    "    # column separator\n",
    "    separator = ' '\n",
    "    # the column index for tags\n",
    "    outputColumnIndex = 1\n",
    "    # Read Gold data\n",
    "    observed = evalResult.get_observed(gold, separator, outputColumnIndex)\n",
    "    # Read Predction data\n",
    "    predicted = evalResult.get_predicted(\n",
    "        predicted, separator, outputColumnIndex)\n",
    "    correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = evalResult.compare_observed_to_predicted(\n",
    "        observed, predicted)\n",
    "\n",
    "    return correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f\n",
    "\n",
    "\n",
    "def best_results(results):\n",
    "    highest_entity_f = 0\n",
    "    highest_entity_method = \"\"\n",
    "    highest_sentiment_f = 0\n",
    "    highest_sentiment_method = \"\"\n",
    "\n",
    "    for method, result in results.items():\n",
    "        # method is the method name, result is a dictionary of the method's results\n",
    "        if result[\"entity_f\"] > highest_entity_f:\n",
    "            highest_entity_f = result[\"entity_f\"]\n",
    "            highest_entity_method = method\n",
    "        if result[\"sentiment_f\"] > highest_sentiment_f:\n",
    "            highest_sentiment_f = result[\"sentiment_f\"]\n",
    "            highest_sentiment_method = method\n",
    "\n",
    "    return highest_entity_method, highest_sentiment_method\n",
    "\n",
    "\n",
    "def predict_es():\n",
    "    results = {}\n",
    "    file_path = \"Data\\\\ES\\\\train\"\n",
    "    state_to_idx, observation_to_idx, states, observations, train_data = prepare_data(\n",
    "        file_path)\n",
    "\n",
    "    validation_file_path = \"Data\\\\ES\\\\dev.in\"\n",
    "    with open(validation_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        validation_data = file.read()\n",
    "\n",
    "    gold_file_path = \"Data\\\\ES\\\\dev.out\"\n",
    "\n",
    "    # Define a range of k values to try\n",
    "    k_values = [0.1 * i for i in range(1, 11)]  # k = 0.1, 0.2, ..., 1.0\n",
    "    d_values = [0.01 * i for i in range(1, 101)]  # d = 0.1, 0.2, ..., 1.0\n",
    "\n",
    "    transition_prob = estimate_transmission_parameters(\n",
    "        train_data, states, state_to_idx)\n",
    "\n",
    "    # Iterate through each k value\n",
    "    for k in k_values:\n",
    "        # Estimate emission probabilities with the current k\n",
    "        emission_prob = estimate_emission_parameters(\n",
    "            train_data, states, observations, state_to_idx, observation_to_idx, k=k)\n",
    "\n",
    "        # Run Viterbi on the validation data\n",
    "        predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                                 observation_to_idx, emission_prob, transition_prob)\n",
    "        output_file_path = \"Testing\\\\es.dev.k\" + str(k)\n",
    "        write_predictions_to_file(\n",
    "            output_file_path, predicted_tags, validation_data)\n",
    "\n",
    "        correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "            gold_file_path, output_file_path)\n",
    "\n",
    "        # add results to dictionary\n",
    "\n",
    "        results[f\"laplace_{k}\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                                   \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    # comparing other smoothing methods\n",
    "\n",
    "    # absolute discounting\n",
    "\n",
    "    for d in d_values:\n",
    "        abs_emission_prob = estimate_emission_parameters_absolute_discounting(\n",
    "            train_data, states, observations, state_to_idx, observation_to_idx, d=d)\n",
    "        # Run Viterbi on the validation data\n",
    "        predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                                 observation_to_idx, abs_emission_prob, transition_prob)\n",
    "        output_file_path = \"Testing\\\\es.dev.abs\"+str(d)\n",
    "        write_predictions_to_file(\n",
    "            output_file_path, predicted_tags, validation_data)\n",
    "        correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "            gold_file_path, output_file_path)\n",
    "\n",
    "        results[f\"abs_{d}\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                               \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    # # witten bell smoothing\n",
    "    wb_emission_prob = estimate_emission_parameters_witten_bell(\n",
    "        train_data, states, observations, state_to_idx, observation_to_idx)\n",
    "    predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                             observation_to_idx, wb_emission_prob, transition_prob)\n",
    "    output_file_path = \"Testing\\\\es.dev.wb\"\n",
    "    write_predictions_to_file(\n",
    "        output_file_path, predicted_tags, validation_data)\n",
    "    correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "        gold_file_path, output_file_path)\n",
    "\n",
    "    results[\"wb\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                     \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    highest_entity_method, highest_sentiment_method = best_results(results)\n",
    "\n",
    "    # we know in hindsight after training that the best smoothing method is absolute discounting\n",
    "    _, sigma = highest_entity_method.split(\"_\")\n",
    "    model_b = estimate_emission_parameters_absolute_discounting(\n",
    "        train_data, states, observations, state_to_idx, observation_to_idx, d=float(sigma))  # emission probabilities for model\n",
    "    model_a = transition_prob  # transition probabilities for model\n",
    "\n",
    "    predicted_tags = viterbi(validation_data, states,\n",
    "                             state_to_idx, observation_to_idx, model_b, model_a)\n",
    "\n",
    "    # add to respective folder\n",
    "    output_file_path = \"Data\\\\ES\\\\dev.p4.out\"\n",
    "    write_predictions_to_file(\n",
    "        output_file_path, predicted_tags, validation_data)\n",
    "    correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "        gold_file_path, output_file_path)\n",
    "\n",
    "    # predict tags for test data\n",
    "    with open(\"Test\\\\ES\\\\test.in\", 'r', encoding=\"utf-8\") as f:\n",
    "        test_data = f.read()\n",
    "\n",
    "    test_output_file_path = \"Data\\\\ES\\\\test.p4.out\"\n",
    "\n",
    "    predicted_tags = viterbi(\n",
    "        test_data, states, state_to_idx, observation_to_idx, model_b, model_a)\n",
    "    write_predictions_to_file(test_output_file_path, predicted_tags, test_data)\n",
    "\n",
    "\n",
    "def predict_ru():\n",
    "    results = {}\n",
    "    file_path = \"Data\\\\RU\\\\train\"\n",
    "    state_to_idx, observation_to_idx, states, observations, train_data = prepare_data(\n",
    "        file_path)\n",
    "\n",
    "    validation_file_path = \"Data\\\\RU\\\\dev.in\"\n",
    "    with open(validation_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        validation_data = file.read()\n",
    "\n",
    "    gold_file_path = \"Data\\\\RU\\\\dev.out\"\n",
    "\n",
    "    # Define a range of k values to try\n",
    "    k_values = [0.1 * i for i in range(1, 11)]  # k = 0.1, 0.2, ..., 1.0\n",
    "\n",
    "    transition_prob = estimate_transmission_parameters(\n",
    "        train_data, states, state_to_idx)\n",
    "\n",
    "    # Iterate through each k value\n",
    "    for k in k_values:\n",
    "        # Estimate emission probabilities with the current k\n",
    "        emission_prob = estimate_emission_parameters(\n",
    "            train_data, states, observations, state_to_idx, observation_to_idx, k=k)\n",
    "\n",
    "        # Run Viterbi on the validation data\n",
    "        predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                                 observation_to_idx, emission_prob, transition_prob)\n",
    "        output_file_path = \"Testing\\\\ru.dev.k\" + str(k)\n",
    "        write_predictions_to_file(\n",
    "            output_file_path, predicted_tags, validation_data)\n",
    "\n",
    "        correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "            gold_file_path, output_file_path)\n",
    "\n",
    "        # add results to dictionary\n",
    "\n",
    "        results[f\"laplace_{k}\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                                   \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    # comparing other smoothing methods\n",
    "\n",
    "    # absolute discounting\n",
    "    d_values = [0.01 * i for i in range(1, 101)]  # d = 0.1, 0.2, ..., 1.0\n",
    "    for d in d_values:\n",
    "        abs_emission_prob = estimate_emission_parameters_absolute_discounting(\n",
    "            train_data, states, observations, state_to_idx, observation_to_idx, d=d)\n",
    "        # Run Viterbi on the validation data\n",
    "        predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                                 observation_to_idx, abs_emission_prob, transition_prob)\n",
    "\n",
    "        output_file_path = \"Testing\\\\ru.dev.abs\"+str(d)\n",
    "        write_predictions_to_file(\n",
    "            output_file_path, predicted_tags, validation_data)\n",
    "        correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "            gold_file_path, output_file_path)\n",
    "\n",
    "        results[f\"abs_{d}\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                               \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    # witten bell smoothing\n",
    "    wb_emission_prob = estimate_emission_parameters_witten_bell(\n",
    "        train_data, states, observations, state_to_idx, observation_to_idx)\n",
    "    predicted_tags = viterbi(validation_data, states, state_to_idx,\n",
    "                             observation_to_idx, wb_emission_prob, transition_prob)\n",
    "\n",
    "    output_file_path = \"Testing\\\\ru.dev.wb\"\n",
    "    write_predictions_to_file(\n",
    "        output_file_path, predicted_tags, validation_data)\n",
    "    correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "        gold_file_path, output_file_path)\n",
    "\n",
    "    results[\"wb\"] = {\"correct_entity\": correct_entity, \"correct_sentiment\": correct_sentiment, \"entity_prec\": entity_prec,\n",
    "                     \"entity_rec\": entity_rec, \"entity_f\": entity_f, \"sentiment_prec\": sentiment_prec, \"sentiment_rec\": sentiment_rec, \"sentiment_f\": sentiment_f}\n",
    "\n",
    "    highest_entity_method, highest_sentiment_method = best_results(results)\n",
    "\n",
    "    # we know in hindsight after training that the best smoothing method is absolute discounting\n",
    "    _, sigma = highest_entity_method.split(\"_\")\n",
    "    model_b = estimate_emission_parameters_absolute_discounting(\n",
    "        train_data, states, observations, state_to_idx, observation_to_idx, d=float(sigma))  # emission probabilities for model\n",
    "    model_a = transition_prob  # transition probabilities for model\n",
    "\n",
    "    predicted_tags = viterbi(validation_data, states,\n",
    "                             state_to_idx, observation_to_idx, model_b, model_a)\n",
    "\n",
    "    # add to respective folder\n",
    "    output_file_path = \"Data\\\\RU\\\\dev.p4.out\"\n",
    "    write_predictions_to_file(\n",
    "        output_file_path, predicted_tags, validation_data)\n",
    "    correct_entity, correct_sentiment, entity_prec, entity_rec, entity_f, sentiment_prec, sentiment_rec, sentiment_f = compute_scores(\n",
    "        gold_file_path, output_file_path)\n",
    "\n",
    "    # predict tags for test data\n",
    "    with open(\"Test\\\\RU\\\\test.in\", \"r\", encoding=\"utf-8\") as f:\n",
    "        test_data = f.read()\n",
    "\n",
    "    test_output_file_path = \"Data\\\\RU\\\\test.p4.out\"\n",
    "\n",
    "    predicted_tags = viterbi(\n",
    "        test_data, states, state_to_idx, observation_to_idx, model_b, model_a)\n",
    "    write_predictions_to_file(test_output_file_path, predicted_tags, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_es()\n",
    "predict_ru()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "# For ES\n",
    "\n",
    "### Entity in gold data: 229\n",
    "\n",
    "### Entity in prediction: 214\n",
    "\n",
    "### Correct Entity : 136\n",
    "\n",
    "-   Entity precision: 0.6355\n",
    "-   Entity recall: 0.5939\n",
    "-   Entity F: 0.6140\n",
    "\n",
    "### Correct Sentiment : 106\n",
    "\n",
    "-   Sentiment precision: 0.4953\n",
    "-   Sentiment recall: 0.4629\n",
    "-   Sentiment F: 0.4786\n",
    "\n",
    "# For RU\n",
    "\n",
    "### Entity in gold data: 389\n",
    "\n",
    "### Entity in prediction: 279\n",
    "\n",
    "### Correct Entity : 186\n",
    "\n",
    "-   Entity precision: 0.6667\n",
    "-   Entity recall: 0.4781\n",
    "-   Entity F: 0.5569\n",
    "\n",
    "### Correct Sentiment : 134\n",
    "\n",
    "-   Sentiment precision: 0.4803\n",
    "-   Sentiment recall: 0.3445\n",
    "-   Sentiment F: 0.4012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 214\n",
      "\n",
      "#Correct Entity : 136\n",
      "Entity  precision: 0.6355\n",
      "Entity  recall: 0.5939\n",
      "Entity  F: 0.6140\n",
      "\n",
      "#Correct Sentiment : 106\n",
      "Sentiment  precision: 0.4953\n",
      "Sentiment  recall: 0.4629\n",
      "Sentiment  F: 0.4786\n"
     ]
    }
   ],
   "source": [
    "# ES Evaluation \n",
    "!python EvalScript/evalResult.py ./Data/ES/dev.out ./Data/ES/dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 279\n",
      "\n",
      "#Correct Entity : 186\n",
      "Entity  precision: 0.6667\n",
      "Entity  recall: 0.4781\n",
      "Entity  F: 0.5569\n",
      "\n",
      "#Correct Sentiment : 134\n",
      "Sentiment  precision: 0.4803\n",
      "Sentiment  recall: 0.3445\n",
      "Sentiment  F: 0.4012\n"
     ]
    }
   ],
   "source": [
    "# RU Evaluation \n",
    "!python EvalScript/evalResult.py ./Data/RU/dev.out ./Data/RU/dev.p4.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
