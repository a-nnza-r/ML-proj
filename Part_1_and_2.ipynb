{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "def prepare_data(file_path):\n",
    "    \"\"\"Prepare the raw data for training\n",
    "\n",
    "    Args:\n",
    "        file_path (string): input file path for preparation\n",
    "\n",
    "    Returns:\n",
    "        dictionary: state_to_idx - a mapping of each state to the index of the data.split('\\n')\n",
    "        dictionary: observation_to_idx - a mapping of each observation to the index of the data.split('\\n')\n",
    "        set: states - a set of unique states\n",
    "        set: observations - a set of unique observations\n",
    "        string: data - raw data\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Collect unique states and observations\n",
    "    states = set()\n",
    "    observations = set()\n",
    "    # split the data by new line - essentially split by comments/sentences\n",
    "    for line in data.split('\\n'):\n",
    "        if line:\n",
    "            state = line[line.rfind(' ') + 1:]\n",
    "            observation = line[:line.rfind(' ')]\n",
    "            # split a valid line by space - for observation and state\n",
    "            states.add(state)\n",
    "            observations.add(observation)\n",
    "\n",
    "    observations.add(\"#UNK#\")  # Include special token for unknown words\n",
    "\n",
    "    # Create dictionary for states and observations - key is state or observation, value is index\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "    observation_to_idx = {obs: idx for idx, obs in enumerate(observations)}\n",
    "\n",
    "    # the idx is important for the shape of the emission probabilities matrix (num_states x num_observations)\n",
    "\n",
    "    return state_to_idx, observation_to_idx, states, observations, data\n",
    "\n",
    "\n",
    "def estimate_b(train_data, state_to_idx, observation_to_idx, states, observations):\n",
    "    \"\"\"Estimate emission probabilities\n",
    "\n",
    "    Args:\n",
    "        train_data (string): raw data\n",
    "        state_to_idx (dictionary): a mapping of each state to the index of the data.split('\\n')\n",
    "        observation_to_idx (dictionary): a mapping of each observation to the index of the data.split('\\n')\n",
    "        states (set): a set of unique states\n",
    "        observations (set): a set of unique observations\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: emission_probabilities - a 2d array of emission probabilities (num_states x num_observations)\n",
    "        note here that emission probabilities have the rows as the states and columns as the observations\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    # Initialize counters\n",
    "    state_counts = np.zeros(len(states))  # count(y)\n",
    "    # 2d array of emission counts (num_states x num_observations) for count(y -> x)\n",
    "    emission_counts = np.zeros((len(states), len(observations)))\n",
    "\n",
    "    # Count occurrences\n",
    "    for line in train_data.split('\\n'):  # get each line of the data\n",
    "        if line:  # if the line is not empty\n",
    "            state = line[line.rfind(' ') + 1:]\n",
    "            observation = line[:line.rfind(' ')]\n",
    "            # get the row index for this observation's state\n",
    "            row_index = state_to_idx[state]\n",
    "            # get the column index for this observation\n",
    "            column_index = observation_to_idx[observation]\n",
    "\n",
    "            # increase the number of occurrences for the state in the data\n",
    "            state_counts[row_index] += 1\n",
    "            # increase the number of occurrences for the observation with this state.\n",
    "            emission_counts[row_index][column_index] += 1\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_probabilities = (emission_counts) / (state_counts[:, None] + k)\n",
    "\n",
    "    # Calculate for unknown words\n",
    "    emission_probabilities[:, observation_to_idx[\"#UNK#\"]\n",
    "                           ] = k / (state_counts + k)\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def predict(test_data, emission_probabilities, observation_to_idx, state_to_idx):\n",
    "    \"\"\"Perform sentiment analysis on a sequence of observations\n",
    "\n",
    "    Args:\n",
    "        test_data (string): a sequence of observations\n",
    "        emission_probabilities (np.ndarray): emission probabilities\n",
    "        observation_to_idx (dictionary): a mapping of each observation to the index of the data.split('\\n')\n",
    "        state_to_idx (dictionary): a mapping of each state to the index of the data.split('\\n')\n",
    "\n",
    "    Returns:\n",
    "        list: predicted_tags - a list of predicted tags\n",
    "    \"\"\"\n",
    "    predicted_tags = []\n",
    "    observation_predicted_pairs = []\n",
    "    for line in test_data.split('\\n'):\n",
    "        if line:  # if the line is not empty\n",
    "            # remove the new line character\n",
    "            observation = line.replace('\\n', '')\n",
    "            # get the index of the observation if it exists, otherwise get the index of the unknown token\n",
    "            observation_idx = observation_to_idx.get(\n",
    "                observation, observation_to_idx[\"#UNK#\"])\n",
    "\n",
    "            # get the index of the state with the highest probability\n",
    "            max_prob_state_idx = np.argmax(\n",
    "                emission_probabilities[:, observation_idx])\n",
    "            # predicted_state = [state for state, idx in state_to_idx.items() if idx == max_prob_state_idx][0]\n",
    "\n",
    "            for state, idx in state_to_idx.items():\n",
    "                if idx == max_prob_state_idx:  # if the index of the state is the same as the index of the state with the highest probability\n",
    "                    predicted_state = state\n",
    "                    break\n",
    "\n",
    "            predicted_tags.append(predicted_state)\n",
    "            observation_predicted_pairs.append(\n",
    "                observation + \" \" + predicted_state)\n",
    "        else:\n",
    "            observation_predicted_pairs.append(\"\")\n",
    "\n",
    "    return predicted_tags, observation_predicted_pairs\n",
    "\n",
    "\n",
    "# def calculate_metrics(predicted_tags, gold_tags):\n",
    "#     \"\"\"Calculate precision, recall and F-score\n",
    "\n",
    "#     Args:\n",
    "#         predicted_tags (list): a nested list of predicted tags\n",
    "#         gold_tags (list): list of gold standard tags (reference output)\n",
    "\n",
    "#     Returns:\n",
    "#         float: Precision\n",
    "#         float: Recall\n",
    "#         float: F-score\n",
    "#     \"\"\"\n",
    "#     correct_entities = 0\n",
    "#     predicted_entities = 0\n",
    "#     gold_entities = 0\n",
    "\n",
    "#     predicted_entity_start = False\n",
    "#     predicted_gold_start = False\n",
    "\n",
    "#     for i in range(len(predicted_tags)):\n",
    "#         predicted_tag = predicted_tags[i]\n",
    "#         gold_tag = gold_tags[i]\n",
    "\n",
    "#         # current tags\n",
    "#         if gold_tag != 'O':\n",
    "#             # add correct entities\n",
    "#             if gold_tag == predicted_tag:\n",
    "#                 correct_entities += 1\n",
    "\n",
    "#             # check if an entity has started\n",
    "#             if gold_tag == \"B-positive\" or gold_tag == \"B-negative\" or gold_tag == \"B-neutral\":\n",
    "#                 gold_entities += 1\n",
    "#                 predicted_gold_start = True\n",
    "\n",
    "#             # check if an entity has started without a B tag\n",
    "#             elif (gold_tag == \"I-positive\" or gold_tag == \"I-negative\" or gold_tag == \"I-neutral\") and predicted_gold_start == False:\n",
    "#                 gold_entities += 1\n",
    "#                 predicted_gold_start = True\n",
    "#         else:\n",
    "#             predicted_gold_start = False  # reset the flag if the current tag is 'O'\n",
    "\n",
    "#         if predicted_tag != 'O':\n",
    "#             # check if an entity has started\n",
    "#             if predicted_tag == \"B-positive\" or predicted_tag == \"B-negative\" or predicted_tag == \"B-neutral\":\n",
    "#                 predicted_entities += 1\n",
    "#                 predicted_entity_start = True\n",
    "\n",
    "#              # check if an entity has started without a B tag\n",
    "#             elif (predicted_tag == \"I-positive\" or predicted_tag == \"I-negative\" or predicted_tag == \"I-neutral\") and predicted_entity_start == False:\n",
    "#                 predicted_entities += 1\n",
    "#                 predicted_entity_start = True\n",
    "#         else:\n",
    "#             predicted_entity_start = False\n",
    "\n",
    "#     Precision = correct_entities / predicted_entities\n",
    "#     Recall = correct_entities / gold_entities\n",
    "#     F = 2 / (1/Precision + 1/Recall)\n",
    "\n",
    "#     return Precision, Recall, F, correct_entities, predicted_entities, gold_entities\n",
    "\n",
    "\n",
    "def gold_labels(data):\n",
    "    \"\"\"Get the gold labels from the data\n",
    "\n",
    "    Args:\n",
    "        data (string): raw data\n",
    "\n",
    "    Returns:\n",
    "        list: gold_labels - a list of gold labels\n",
    "    \"\"\"\n",
    "    gold_labels = []\n",
    "    for line in data.split('\\n'):\n",
    "        if line:\n",
    "            observation, state = line.split(' ')\n",
    "            gold_labels.append(state)\n",
    "    return gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ES dataset\n",
    "\n",
    "# prepare data\n",
    "es_state_to_idx, es_observation_to_idx, es_states, es_observations, es_train_data = prepare_data(\n",
    "    \".\\\\ES\\\\train\")\n",
    "\n",
    "# probability of each state\n",
    "es_emission_probabilities = estimate_b(\n",
    "    es_train_data, es_state_to_idx, es_observation_to_idx, es_states, es_observations)\n",
    "\n",
    "\n",
    "# predict\n",
    "with open(\".\\\\ES\\\\dev.in\", \"r\") as f:\n",
    "    es_test_data = f.read()  # read in the file as a string\n",
    "\n",
    "# predict the states for the test data\n",
    "es_predicted_states, es_observation_predicted_tags = predict(\n",
    "    es_test_data, es_emission_probabilities, es_observation_to_idx, es_state_to_idx)\n",
    "\n",
    "\n",
    "# Write to dev.p1.out\n",
    "with open(\".\\\\ES\\\\dev.p1.out\", \"w\") as f:\n",
    "    for i in range(len(es_observation_predicted_tags)):\n",
    "        f.write(es_observation_predicted_tags[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RU dataset\n",
    "\n",
    "# prepare data\n",
    "ru_state_to_idx, ru_observation_to_idx, ru_states, ru_observations, ru_train_data = prepare_data(\n",
    "    \".\\\\RU\\\\train\")\n",
    "\n",
    "\n",
    "# probability of each state\n",
    "ru_emission_probabilities = estimate_b(\n",
    "    ru_train_data, ru_state_to_idx, ru_observation_to_idx, ru_states, ru_observations)\n",
    "\n",
    "\n",
    "# predict\n",
    "with open(\".\\\\RU\\\\dev.in\", \"r\") as f:\n",
    "    ru_test_data = f.read()  # read in the file as a string\n",
    "\n",
    "# predict the states for the test data\n",
    "ru_predicted_states, ru_observation_predicted_tags = predict(\n",
    "    ru_test_data, ru_emission_probabilities, ru_observation_to_idx, ru_state_to_idx)\n",
    "\n",
    "\n",
    "# Write to dev.p1.out\n",
    "with open(\".\\\\RU\\\\dev.p1.out\", \"w\") as f:\n",
    "    for i in range(len(ru_observation_predicted_tags)):\n",
    "        f.write(ru_observation_predicted_tags[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for ES\n",
    "\n",
    "#### Entity in gold data: 229\n",
    "\n",
    "#### Entity in prediction: 1466\n",
    "\n",
    "#### Correct Entity : 178\n",
    "\n",
    "-   Entity precision: 0.1214\n",
    "-   Entity recall: 0.7773\n",
    "-   Entity F: 0.2100\n",
    "\n",
    "#### Correct Sentiment : 97\n",
    "\n",
    "-   Sentiment precision: 0.0662\n",
    "-   Sentiment recall: 0.4236\n",
    "-   Sentiment F: 0.1145\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluation for RU\n",
    "\n",
    "#### Entity in gold data: 389\n",
    "\n",
    "#### Entity in prediction: 1816\n",
    "\n",
    "#### Correct Entity : 266\n",
    "\n",
    "-   Entity precision: 0.1465\n",
    "-   Entity recall: 0.6838\n",
    "-   Entity F: 0.2413\n",
    "\n",
    "#### Correct Sentiment : 129\n",
    "\n",
    "-   Sentiment precision: 0.0710\n",
    "-   Sentiment recall: 0.3316\n",
    "-   Sentiment F: 0.1170\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def prepare_transition_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    states = set() # a unique set of states\n",
    "    states.add(\"START\")\n",
    "    \n",
    "    for line in data.split(\"\\n\"):\n",
    "        if line:\n",
    "            state = line[line.rfind(\" \") + 1:]\n",
    "            states.add(state) \n",
    "\n",
    "    states.add(\"STOP\")\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)} # a dictionary mapping states to indices\n",
    "    return states, state_to_idx, data\n",
    "\n",
    "\n",
    "\n",
    "def estimate_a(train_data, state_to_idx, states):\n",
    "    number_of_states = len(states)\n",
    "    transitions = np.zeros((number_of_states, number_of_states)) # a matrix that counts the number of transitions: count(u; v)\n",
    "    total_state_counts = np.zeros(number_of_states) # counts the total number of states in the data: count(u)      \n",
    "    \n",
    "    # count occurences\n",
    "    start = True\n",
    "    for line in train_data.split(\"\\n\"):\n",
    "        if line and start: # beginning of a section\n",
    "            u = \"START\"\n",
    "            v = line[line.rfind(\" \") + 1:]\n",
    "            start = False\n",
    "            prev = v\n",
    "            \n",
    "        elif line and not start: # middle of a section\n",
    "            u = prev\n",
    "            v = line[line.rfind(\" \") + 1:]\n",
    "            prev = v\n",
    "        \n",
    "        else: # end of a section - indicated by empty line\n",
    "            u = prev\n",
    "            v = \"STOP\"\n",
    "            start = True\n",
    "            \n",
    "        # update the counts\n",
    "        # print(u,v)\n",
    "        transitions[state_to_idx[u], state_to_idx[v]] += 1\n",
    "        total_state_counts[state_to_idx[u]] += 1\n",
    "    \n",
    "    # estimate transition probabilities\n",
    "    a = np.zeros((number_of_states, number_of_states))\n",
    "    for u in states:\n",
    "        for v in states:\n",
    "            # q(yi|yi−1) = Count(yi−1, yi) / Count(yi−1)\n",
    "            \n",
    "            # account for division by 0\n",
    "            if total_state_counts[state_to_idx[u]] == 0:\n",
    "                a[state_to_idx[u], state_to_idx[v]] = 0\n",
    "            else:\n",
    "                a[state_to_idx[u], state_to_idx[v]] = transitions[state_to_idx[u], state_to_idx[v]] / total_state_counts[state_to_idx[u]]\n",
    "    return a\n",
    "\n",
    "\n",
    "def r(tags, state_to_idx, observation_to_idx, emission_probabilities, transition_probabilities):\n",
    "    \"\"\"calculate joint probability of a sequence of tags\n",
    "\n",
    "    Args:\n",
    "        tags (list): a sequence of tags\n",
    "        state_to_idx (numpy.ndarray): a dictionary mapping states to indices\n",
    "        observation_to_idx (numpy.ndarray): a dictionary mapping observations to indices\n",
    "        emission_probabilities (numpy.ndarray): 2D matrix of emission probabilities\n",
    "        transition_probabilities (numpy.ndarray): 2D matrix transition probabilities\n",
    "    \"\"\"\n",
    "    for i in range(len(tags)):\n",
    "        y = tags[i]\n",
    "        if i == 0:\n",
    "            p = transition_probabilities[state_to_idx[\"START\"], state_to_idx[y]] * emission_probabilities[state_to_idx[y], observation_to_idx[y]]\n",
    "        else:\n",
    "            y_prev = tags[i-1]\n",
    "            p *= transition_probabilities[state_to_idx[y_prev], state_to_idx[y]] * emission_probabilities[state_to_idx[y], observation_to_idx[y]]\n",
    "            \n",
    "    return p\n",
    "    \n",
    "def viterbi(transition_prob, emission_prob, state_to_idx, state_to_idx_observations, observation_to_idx, observations):\n",
    "    \"\"\"Use Viterbi algorithm to find the most likely sequence of states\n",
    "\n",
    "    Args:\n",
    "        transition_prob (numpy.ndarray): 2D matrix of transition probabilities\n",
    "        emission_prob (numpy.ndarray): 2D matrix of emission probabilities\n",
    "        state_to_idx (dictionary): a dictionary mapping states (does not include start and stop) to indices\n",
    "        observation_to_idx (dictionary): a dictionary mapping observations to indices\n",
    "        observations (list): a sequence of observations from the train data\n",
    "        \n",
    "    Returns:\n",
    "        best_path_states (list): the most likely sequence of states for the observations\n",
    "    \"\"\"\n",
    "    number_of_observations = len(observations)\n",
    "    \n",
    "    # initialize tabulation table for bottom up dynamic programming, + 2 to account for start and stop\n",
    "    dp = np.zeros((number_of_observations+2, len(state_to_idx))) # (number of observations, number of states) - dp[i, j] = most likely sequence of states up to observation i with state j\n",
    "    \n",
    "    start_idx = state_to_idx[\"START\"]\n",
    "    stop_idx = state_to_idx[\"STOP\"]\n",
    "    \n",
    "    \n",
    "    # (step 1)\n",
    "    # Base case - k = 0 is the START where the first observation is k = 1 -- each row in the dp table is an observation\n",
    "    dp[0, start_idx] = 1 # start state\n",
    "    \n",
    "    \n",
    "    # (step 2)\n",
    "    for j in range(1, number_of_observations-2): # number_of_observations - 2 because we don't want to include the last row which is STOP\n",
    "        for u in state_to_idx:\n",
    "            u_idx = state_to_idx[u] # index of state u which is the column index, u is the previous state\n",
    "            for v in state_to_idx:\n",
    "                if v == \"START\" or v == \"STOP\": # skip start and stop states\n",
    "                    continue\n",
    "                observation_idx = observation_to_idx[observations[j]] # index of observation\n",
    "                v_idx = state_to_idx[v] # index of state v which is the column index, v is the current state\n",
    "                v_idx_observations = state_to_idx_observations[v] # index of state v for the observation_to_idx matrix\n",
    "                # recurrence relation\n",
    "                dp[j, v_idx] = max(dp[j, v_idx], dp[j-1, u_idx] * emission_prob[v_idx_observations, observation_idx] * transition_prob[u_idx, v_idx] )\n",
    "    \n",
    "    # (step 3)        \n",
    "    # account for stop state\n",
    "    for i in range(len(dp[number_of_observations-2])): # iterate through the second to last row\n",
    "        if i == stop_idx or i == start_idx: # skip start and stop states\n",
    "            continue\n",
    "        dp[number_of_observations-1, stop_idx] = max(dp[number_of_observations-1, stop_idx], dp[number_of_observations-2, i] * transition_prob[i, stop_idx])\n",
    "    \n",
    "    # output predicted sequence of states\n",
    "    print(dp)\n",
    "    best_path_states = []\n",
    "    for index in range(dp.shape[0]):\n",
    "        best_path_states.append(np.argmax(dp[index]))\n",
    "    \n",
    "    return best_path_states\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [0.00000000e+00 8.31790117e-04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.39542432e-06 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 3.06456350e-09 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.21518337e-12 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 3.79552291e-14 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 4.16777207e-17 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.27125830e-20 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 3.97066822e-22 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 4.84454989e-26 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.69934326e-28 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 2.05260859e-30 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.03304664e-32 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.45420863e-35\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.24328050e-34 0.00000000e+00 4.76911968e-38\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 6.23731303e-37 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.43639648e-38 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.40201982e-41 0.00000000e+00 0.00000000e+00\n",
      "  7.81540444e-42 3.78136748e-42 0.00000000e+00 7.63304628e-41\n",
      "  0.00000000e+00]\n",
      " [2.43292209e-44 3.81248020e-42 0.00000000e+00 5.64018198e-44\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.70400500e-46 7.46573321e-44 0.00000000e+00 3.06215522e-46\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.96737778e-49 5.46529425e-47 0.00000000e+00 1.10833239e-48\n",
      "  1.43367898e-47 0.00000000e+00 0.00000000e+00 1.40851207e-47\n",
      "  0.00000000e+00]\n",
      " [4.46301826e-50 2.77393776e-48 0.00000000e+00 1.04077247e-50\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [3.12587298e-52 5.43202278e-50 0.00000000e+00 5.65053903e-53\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 2.15394583e-53 0.00000000e+00 1.02259275e-55\n",
      "  1.73856041e-54 1.58888924e-54 0.00000000e+00 7.51538298e-53\n",
      "  0.00000000e+00]\n",
      " [5.41210894e-57 3.75371087e-54 0.00000000e+00 5.55323866e-56\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.68591617e-59 8.64445486e-56 0.00000000e+00 1.20598085e-57\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.27348176e-60 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.90270960e-59\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 2.57004899e-62 0.00000000e+00 0.00000000e+00\n",
      "  1.68781814e-64 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.14452188e-64 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 3.84013618e-68 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.34702082e-70 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "[8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, 7, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.int64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# write to file\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(predicted_states)):\n\u001b[1;32m---> 23\u001b[0m     f\u001b[39m.\u001b[39mwrite(observations[i] \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m predicted_states[i] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m observations \u001b[39m=\u001b[39m [] \u001b[39m# reset observations\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.int64\") to str"
     ]
    }
   ],
   "source": [
    "# for ES\n",
    "\n",
    "es_t_states, es_t_state_to_idx, es_train_data = prepare_transition_data(\".\\\\ES\\\\train\")\n",
    "\n",
    "\n",
    "es_transition_probabilities = estimate_a(es_train_data, es_t_state_to_idx, es_t_states) # the state to idx should not include start and stop to ensure correct indices\n",
    "\n",
    "# write to dev.p2.out\n",
    "\n",
    "es_train_data = es_train_data.split(\"\\n\")\n",
    "\n",
    "observations = []\n",
    "\n",
    "with open(\".\\\\ES\\\\dev.p2.out\", \"w\") as f:\n",
    "    # grab each chunk and use viterbi to predict the sequence of tags\n",
    "    for line in es_train_data:\n",
    "        if not line:\n",
    "            # empty line indicates the end of a sentence\n",
    "            predicted_states = viterbi(es_transition_probabilities, es_emission_probabilities, es_t_state_to_idx, es_state_to_idx, es_observation_to_idx, observations)\n",
    "            print(predicted_states)\n",
    "            # write to file\n",
    "            for i in range(len(predicted_states)):\n",
    "                f.write(observations[i] + \" \" + predicted_states[i] + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            observations = [] # reset observations\n",
    "        \n",
    "        else:\n",
    "            word, tag = line.split(\" \")\n",
    "            observations.append(word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
