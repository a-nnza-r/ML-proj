{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6913e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8c187",
   "metadata": {},
   "source": [
    "# Q1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00103868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that estimates the emission parameters from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5c1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    " def emission_parameters_v1(training_filepath):\n",
    "    with open(training_filepath, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the entire contents of the file\n",
    "        training_contents = file.read().split(\"\\n\")\n",
    "        \n",
    "    # Convert the data into a Pandas Dataframe\n",
    "    training_words = []\n",
    "    training_labels = []\n",
    "    # print(training_contents)\n",
    "    \n",
    "    for line in training_contents:\n",
    "        # Exclude spaces in the dataset in training\n",
    "        if len(line) > 0:\n",
    "            parts = line.split()  # Split the line by spaces\n",
    "            # To account for RUS cases wher there are spaces in the words\n",
    "            if len(parts) >= 3:    \n",
    "                word = \" \".join(parts[:-2])  # Join the parts to get the word\n",
    "                label = parts[-1]            # The last part is the label\n",
    "                training_words.append(word)\n",
    "                training_labels.append(label)\n",
    "            else:\n",
    "                training_words.append(parts[0])\n",
    "                training_labels.append(parts[1])\n",
    "\n",
    "#     # Create a pandas DataFrame\n",
    "#     training_data = {\n",
    "#         \"x\": training_words,\n",
    "#         \"y\": training_labels\n",
    "#     }\n",
    "\n",
    "#     training_df = pd.DataFrame(training_data)\n",
    "#     # print(training_df)\n",
    "\n",
    "#    # Now we want to calculate the emission parameter for each word-label pair\n",
    "#     emission_parameters = {}\n",
    "    \n",
    "#     for label in training_df[\"y\"].unique()[0:7]:\n",
    "#         label_df = training_df[training_df[\"y\"] == label]\n",
    "#         total_label_occurrences = len(label_df)\n",
    "        \n",
    "#         word_counts = label_df[\"x\"].value_counts().to_dict()\n",
    "#         emission_probabilities = {word: count / total_label_occurrences for word, count in word_counts.items()}\n",
    "        \n",
    "#         emission_parameters[label] = emission_probabilities\n",
    "    \n",
    "#     return emission_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f14ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    " def emission_parameters_v2(training_filepath):\n",
    "    with open(training_filepath, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the entire contents of the file\n",
    "        training_contents = file.read().split(\"\\n\")\n",
    "        \n",
    "    # Convert the data into a Pandas Dataframe\n",
    "    training_words = []\n",
    "    training_labels = []\n",
    "    # print(training_contents)\n",
    "    \n",
    "    for line in training_contents:\n",
    "        # Exclude spaces in the dataset in training\n",
    "        if len(line) > 0:\n",
    "            parts = line.split()  # Split the line by spaces\n",
    "            # print(parts)\n",
    "            # To account for RUS cases wher there are spaces in the words\n",
    "            if len(parts) >= 3:    \n",
    "                word = \" \".join(parts[:-2])  # Join the parts to get the word\n",
    "                label = parts[-1]            # The last part is the label\n",
    "                training_words.append(word)\n",
    "                training_labels.append(label)\n",
    "            else:\n",
    "                training_words.append(parts[0])\n",
    "                training_labels.append(parts[1])\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    training_data = {\n",
    "        \"x\": training_words,\n",
    "        \"y\": training_labels\n",
    "    }\n",
    "    # print(len(training_words) ,len(training_labels))\n",
    "\n",
    "    training_df = pd.DataFrame(training_data)\n",
    "\n",
    "   # Now we want to calculate the emission parameter for each word-label pair\n",
    "    emission_parameters = {}\n",
    "    \n",
    "    for label in training_df[\"y\"].unique()[0:7]:\n",
    "        label_df = training_df[training_df[\"y\"] == label]\n",
    "        total_label_occurrences = len(label_df)\n",
    "        \n",
    "        word_counts = label_df[\"x\"].value_counts().to_dict()\n",
    "        \n",
    "        # Add #UNK# special word here with a count of 1 for ALL labels\n",
    "        word_counts[\"#UNK#\"] = 1\n",
    "        \n",
    "        # Update emission_probabilities denominator\n",
    "        emission_probabilities = {word: count / (total_label_occurrences+1) for word, count in word_counts.items()}\n",
    "        \n",
    "        emission_parameters[label] = emission_probabilities\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "def label_test_set(testing_filepath, emission_params):\n",
    "    # Pulling out the test set\n",
    "    with open(testing_filepath, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the entire contents of the file\n",
    "        test_words = file.read().split('\\n')\n",
    "    \n",
    "    labelled_sequence = []\n",
    "    for word in test_words:\n",
    "        \n",
    "        # if len(word) == 0:\n",
    "        #     labelled_sequence.append(\"Space\")\n",
    "        # else:\n",
    "        #     max_emission_prob = 0.0\n",
    "        #     for label, word_emission_probs in emission_params.items():\n",
    "        #         if word in word_emission_probs:\n",
    "        #             if word_emission_probs[word] > max_emission_prob:\n",
    "        #                 max_emission_prob = word_emission_probs[word]\n",
    "        #                 max_label = label\n",
    "                        \n",
    "        #         # Word does not exist in the dataset\n",
    "        #         else:\n",
    "        #             if word_emission_probs[\"#UNK#\"] > max_emission_prob:\n",
    "        #                 max_emission_prob = word_emission_probs[\"#UNK#\"]\n",
    "        #                 max_label = label\n",
    "\n",
    "        #     labelled_sequence.append(max_label)\n",
    "        \n",
    "        if len(word) == 0:\n",
    "            labelled_sequence.append(\"Space\")\n",
    "        else:\n",
    "            # Check if word has already been trained in the dataset\n",
    "            exists = False\n",
    "            \n",
    "            for label, word_emission_probs in emission_params.items():\n",
    "                if word in word_emission_probs:\n",
    "                    exists = True\n",
    "                    \n",
    "            if exists:\n",
    "                max_emission_prob = 0.0\n",
    "                for label, word_emission_probs in emission_params.items():\n",
    "                    if word in word_emission_probs:\n",
    "                        if word_emission_probs[word] > max_emission_prob:\n",
    "                            max_emission_prob = word_emission_probs[word]\n",
    "                            max_label = label\n",
    "            else:\n",
    "                max_emission_prob = 0.0\n",
    "                for label, word_emission_probs in emission_params.items():\n",
    "                    if word_emission_probs[\"#UNK#\"] > max_emission_prob:\n",
    "                        max_emission_prob = word_emission_probs[\"#UNK#\"]\n",
    "                        max_label = label\n",
    "            \n",
    "            labelled_sequence.append(max_label)\n",
    "\n",
    "    return labelled_sequence\n",
    "\n",
    "def write_predictions_to_file(testing_filepath, test_labels, output_filepath):\n",
    "    # Pulling out the test set\n",
    "    with open(testing_filepath, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the entire contents of the file\n",
    "        test_words = file.read().split('\\n')\n",
    "    with open(output_filepath, \"w\", encoding=\"UTF-8\") as file:\n",
    "        for word, label in zip(test_words, test_labels):\n",
    "            if label == \"Space\":\n",
    "                file.write(\"\\n\")\n",
    "            else:\n",
    "                file.write(f\"{word} {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588cac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES Dataset Prediction\n",
    "ES_training_emission_params_v2 = emission_parameters_v2(training_filepath=\"Data/ES/train\")\n",
    "predicted_labels = label_test_set(testing_filepath=\"Data/ES/dev.in\", emission_params=ES_training_emission_params_v2)\n",
    "write_predictions_to_file(testing_filepath=\"Data/ES/dev.in\", test_labels=predicted_labels,output_filepath=\"Data/ES/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8190570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1466\n",
      "\n",
      "#Correct Entity : 178\n",
      "Entity  precision: 0.1214\n",
      "Entity  recall: 0.7773\n",
      "Entity  F: 0.2100\n",
      "\n",
      "#Correct Sentiment : 97\n",
      "Sentiment  precision: 0.0662\n",
      "Sentiment  recall: 0.4236\n",
      "Sentiment  F: 0.1145\n"
     ]
    }
   ],
   "source": [
    "# Now perform ES evaluation with given script\n",
    "!python EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RU Dataset Prediction\n",
    "RU_training_emission_params_v2 = emission_parameters_v2(training_filepath=\"Data/RU/train\")\n",
    "predicted_labels = label_test_set(testing_filepath=\"Data/RU/dev.in\", emission_params=RU_training_emission_params_v2)\n",
    "write_predictions_to_file(testing_filepath=\"Data/RU/dev.in\", test_labels=predicted_labels,output_filepath=\"Data/RU/dev.p1.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef527db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1816\n",
      "\n",
      "#Correct Entity : 266\n",
      "Entity  precision: 0.1465\n",
      "Entity  recall: 0.6838\n",
      "Entity  F: 0.2413\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.0710\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.1170\n"
     ]
    }
   ],
   "source": [
    "!python EvalScript/evalResult.py Data/RU/dev.out Data/RU/dev.p1.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
