{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "def prepare_data(file_path):\n",
    "    \"\"\"Prepare the raw data for training\n",
    "\n",
    "    Args:\n",
    "        file_path (string): input file path for preparation\n",
    "\n",
    "    Returns:\n",
    "        dictionary: state_to_idx - a mapping of each state to the index of the data.split('\\n')\n",
    "        dictionary: observation_to_idx - a mapping of each observation to the index of the data.split('\\n')\n",
    "        set: states - a set of unique states\n",
    "        set: observations - a set of unique observations\n",
    "        string: data - raw data\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "        # Collect unique states and observations\n",
    "        states = set()\n",
    "        observations = set()\n",
    "        # split the data by new line - essentially split by comments/sentences\n",
    "        for line in data.split('\\n'):\n",
    "            if line:\n",
    "                state = line[line.rfind(' ') + 1:]\n",
    "                observation = line[:line.rfind(' ')]\n",
    "                # split a valid line by space - for observation and state\n",
    "                states.add(state)\n",
    "                observations.add(observation)\n",
    "\n",
    "        observations.add(\"#UNK#\")  # Include special token for unknown words\n",
    "\n",
    "    # Create dictionary for states and observations - key is state or observation, value is index\n",
    "    state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "    observation_to_idx = {obs: idx for idx, obs in enumerate(observations)}\n",
    "\n",
    "    # the idx is important for the shape of the emission probabilities matrix (num_states x num_observations)\n",
    "\n",
    "    return state_to_idx, observation_to_idx, states, observations, data\n",
    "\n",
    "\n",
    "def estimate_b(train_data, state_to_idx, observation_to_idx, states, observations):\n",
    "    \"\"\"Estimate emission probabilities\n",
    "\n",
    "    Args:\n",
    "        train_data (string): raw data\n",
    "        state_to_idx (dictionary): a mapping of each state to the index of the data.split('\\n')\n",
    "        observation_to_idx (dictionary): a mapping of each observation to the index of the data.split('\\n')\n",
    "        states (set): a set of unique states\n",
    "        observations (set): a set of unique observations\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: emission_probabilities - a 2d array of emission probabilities (num_states x num_observations)\n",
    "        note here that emission probabilities have the rows as the states and columns as the observations\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    # Initialize counters\n",
    "    state_counts = np.zeros(len(states))  # count(y)\n",
    "    # 2d array of emission counts (num_states x num_observations) for count(y -> x)\n",
    "    emission_counts = np.zeros((len(states), len(observations)))\n",
    "\n",
    "    # Count occurrences\n",
    "    for line in train_data.split('\\n'):  # get each line of the data\n",
    "        if line:  # if the line is not empty\n",
    "            state = line[line.rfind(' ') + 1:]\n",
    "            observation = line[:line.rfind(' ')]\n",
    "            # get the row index for this observation's state\n",
    "            row_index = state_to_idx[state]\n",
    "            # get the column index for this observation\n",
    "            column_index = observation_to_idx[observation]\n",
    "\n",
    "            # increase the number of occurrences for the state in the data\n",
    "            state_counts[row_index] += 1\n",
    "            # increase the number of occurrences for the observation with this state.\n",
    "            emission_counts[row_index][column_index] += 1\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_probabilities = (emission_counts) / (state_counts[:, None] + k)\n",
    "\n",
    "    # Calculate for unknown words\n",
    "    emission_probabilities[:, observation_to_idx[\"#UNK#\"]\n",
    "                           ] = k / (state_counts + k)\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def predict(test_data, emission_probabilities, observation_to_idx, state_to_idx):\n",
    "    \"\"\"Perform sentiment analysis on a sequence of observations\n",
    "\n",
    "    Args:\n",
    "        test_data (string): a sequence of observations\n",
    "        emission_probabilities (np.ndarray): emission probabilities\n",
    "        observation_to_idx (dictionary): a mapping of each observation to the index of the data.split('\\n')\n",
    "        state_to_idx (dictionary): a mapping of each state to the index of the data.split('\\n')\n",
    "\n",
    "    Returns:\n",
    "        list: predicted_tags - a list of predicted tags\n",
    "    \"\"\"\n",
    "    predicted_tags = []\n",
    "    observation_predicted_pairs = []\n",
    "    for line in test_data.split('\\n'):\n",
    "        if line:  # if the line is not empty\n",
    "            # remove the new line character\n",
    "            observation = line.replace('\\n', '')\n",
    "            # get the index of the observation if it exists, otherwise get the index of the unknown token\n",
    "            observation_idx = observation_to_idx.get(\n",
    "                observation, observation_to_idx[\"#UNK#\"])\n",
    "\n",
    "            # get the index of the state with the highest probability\n",
    "            max_prob_state_idx = np.argmax(\n",
    "                emission_probabilities[:, observation_idx])\n",
    "            # predicted_state = [state for state, idx in state_to_idx.items() if idx == max_prob_state_idx][0]\n",
    "\n",
    "            for state, idx in state_to_idx.items():\n",
    "                if idx == max_prob_state_idx:  # if the index of the state is the same as the index of the state with the highest probability\n",
    "                    predicted_state = state\n",
    "                    break\n",
    "\n",
    "            predicted_tags.append(predicted_state)\n",
    "            observation_predicted_pairs.append(\n",
    "                observation + \" \" + predicted_state)\n",
    "        else:\n",
    "            observation_predicted_pairs.append(\"\")\n",
    "\n",
    "    return predicted_tags, observation_predicted_pairs\n",
    "\n",
    "\n",
    "def calculate_metrics(predicted_tags, gold_tags):\n",
    "    \"\"\"Calculate precision, recall and F-score\n",
    "\n",
    "    Args:\n",
    "        predicted_tags (list): a nested list of predicted tags\n",
    "        gold_tags (list): list of gold standard tags (reference output)\n",
    "\n",
    "    Returns:\n",
    "        float: Precision\n",
    "        float: Recall\n",
    "        float: F-score\n",
    "    \"\"\"\n",
    "    correct_entities = 0\n",
    "    predicted_entities = 0\n",
    "    gold_entities = 0\n",
    "\n",
    "    predicted_entity_start = False\n",
    "    predicted_gold_start = False\n",
    "\n",
    "    for i in range(len(predicted_tags)):\n",
    "        predicted_tag = predicted_tags[i]\n",
    "        gold_tag = gold_tags[i]\n",
    "\n",
    "        # current tags\n",
    "        if gold_tag != 'O':\n",
    "            # add correct entities\n",
    "            if gold_tag == predicted_tag:\n",
    "                correct_entities += 1\n",
    "\n",
    "            # check if an entity has started\n",
    "            if gold_tag == \"B-positive\" or gold_tag == \"B-negative\" or gold_tag == \"B-neutral\":\n",
    "                gold_entities += 1\n",
    "                predicted_gold_start = True\n",
    "\n",
    "            # check if an entity has started without a B tag\n",
    "            elif (gold_tag == \"I-positive\" or gold_tag == \"I-negative\" or gold_tag == \"I-neutral\") and predicted_gold_start == False:\n",
    "                gold_entities += 1\n",
    "                predicted_gold_start = True\n",
    "        else:\n",
    "            predicted_gold_start = False  # reset the flag if the current tag is 'O'\n",
    "\n",
    "        if predicted_tag != 'O':\n",
    "            # check if an entity has started\n",
    "            if predicted_tag == \"B-positive\" or predicted_tag == \"B-negative\" or predicted_tag == \"B-neutral\":\n",
    "                predicted_entities += 1\n",
    "                predicted_entity_start = True\n",
    "\n",
    "             # check if an entity has started without a B tag\n",
    "            elif (predicted_tag == \"I-positive\" or predicted_tag == \"I-negative\" or predicted_tag == \"I-neutral\") and predicted_entity_start == False:\n",
    "                predicted_entities += 1\n",
    "                predicted_entity_start = True\n",
    "        else:\n",
    "            predicted_entity_start = False\n",
    "\n",
    "    Precision = correct_entities / predicted_entities\n",
    "    Recall = correct_entities / gold_entities\n",
    "    F = 2 / (1/Precision + 1/Recall)\n",
    "\n",
    "    return Precision, Recall, F, correct_entities, predicted_entities, gold_entities\n",
    "\n",
    "\n",
    "def gold_labels(data):\n",
    "    \"\"\"Get the gold labels from the data\n",
    "\n",
    "    Args:\n",
    "        data (string): raw data\n",
    "\n",
    "    Returns:\n",
    "        list: gold_labels - a list of gold labels\n",
    "    \"\"\"\n",
    "    gold_labels = []\n",
    "    for line in data.split('\\n'):\n",
    "        if line:\n",
    "            observation, state = line.split(' ')\n",
    "            gold_labels.append(state)\n",
    "    return gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ES dataset\n",
    "\n",
    "# prepare data\n",
    "es_state_to_idx, es_observation_to_idx, es_states, es_observations, es_train_data = prepare_data(\n",
    "    \".\\\\ES\\\\train\")\n",
    "\n",
    "# probability of each state\n",
    "es_emission_probabilities = estimate_b(\n",
    "    es_train_data, es_state_to_idx, es_observation_to_idx, es_states, es_observations)\n",
    "\n",
    "\n",
    "# predict\n",
    "with open(\".\\\\ES\\\\dev.in\", \"r\") as f:\n",
    "    es_test_data = f.read()  # read in the file as a string\n",
    "\n",
    "# predict the states for the test data\n",
    "es_predicted_states, es_observation_predicted_tags = predict(\n",
    "    es_test_data, es_emission_probabilities, es_observation_to_idx, es_state_to_idx)\n",
    "\n",
    "\n",
    "# get gold standard labels\n",
    "# with open(\"C:\\\\Users\\\\nryan\\\\OneDrive - Singapore University of Technology and Design\\\\Term 5\\\\ML\\\\ML Project\\\\ES\\\\dev.out\", \"r\") as f:\n",
    "#     gold_standard = f.read() # read in the file as a string\n",
    "\n",
    "# gold_standard_labels = gold_labels(gold_standard)\n",
    "\n",
    "# precision, recall, f, correct_entities, predicted_entities, gold_entities = calculate_metrics(predicted_states, gold_standard_labels)\n",
    "\n",
    "# print(\"Correct entities: \", correct_entities)\n",
    "# print(\"Predicted entities: \", predicted_entities)\n",
    "# print(\"Gold entities: \", gold_entities)\n",
    "# print()\n",
    "# print(\"Precision: \", precision)\n",
    "# print(\"Recall: \", recall)\n",
    "# print(\"F: \", f)\n",
    "\n",
    "\n",
    "# Write to dev.p1.out\n",
    "with open(\".\\\\ES\\\\dev.p1.out\", \"w\") as f:\n",
    "    for i in range(len(es_observation_predicted_tags)):\n",
    "        f.write(es_observation_predicted_tags[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RU dataset\n",
    "\n",
    "# prepare data\n",
    "ru_state_to_idx, ru_observation_to_idx, ru_states, ru_observations, ru_train_data = prepare_data(\n",
    "    \".\\\\RU\\\\train\")\n",
    "\n",
    "\n",
    "# probability of each state\n",
    "ru_emission_probabilities = estimate_b(\n",
    "    ru_train_data, ru_state_to_idx, ru_observation_to_idx, ru_states, ru_observations)\n",
    "\n",
    "\n",
    "# predict\n",
    "with open(\".\\\\RU\\\\dev.in\", \"r\") as f:\n",
    "    ru_test_data = f.read()  # read in the file as a string\n",
    "\n",
    "# predict the states for the test data\n",
    "ru_predicted_states, ru_observation_predicted_tags = predict(\n",
    "    ru_test_data, ru_emission_probabilities, ru_observation_to_idx, ru_state_to_idx)\n",
    "\n",
    "\n",
    "# get gold standard labels\n",
    "# with open(\"C:\\\\Users\\\\nryan\\\\OneDrive - Singapore University of Technology and Design\\\\Term 5\\\\ML\\\\ML Project\\\\ES\\\\dev.out\", \"r\") as f:\n",
    "#     gold_standard = f.read() # read in the file as a string\n",
    "\n",
    "# gold_standard_labels = gold_labels(gold_standard)\n",
    "\n",
    "# precision, recall, f, correct_entities, predicted_entities, gold_entities = calculate_metrics(predicted_states, gold_standard_labels)\n",
    "\n",
    "# print(\"Correct entities: \", correct_entities)\n",
    "# print(\"Predicted entities: \", predicted_entities)\n",
    "# print(\"Gold entities: \", gold_entities)\n",
    "# print()\n",
    "# print(\"Precision: \", precision)\n",
    "# print(\"Recall: \", recall)\n",
    "# print(\"F: \", f)\n",
    "\n",
    "\n",
    "# Write to dev.p1.out\n",
    "with open(\".\\\\RU\\\\dev.p1.out\", \"w\") as f:\n",
    "    for i in range(len(ru_observation_predicted_tags)):\n",
    "        f.write(ru_observation_predicted_tags[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for ES\n",
    "\n",
    "#### Entity in gold data: 229\n",
    "\n",
    "#### Entity in prediction: 1466\n",
    "\n",
    "#### Correct Entity : 178\n",
    "\n",
    "-   Entity precision: 0.1214\n",
    "-   Entity recall: 0.7773\n",
    "-   Entity F: 0.2100\n",
    "\n",
    "#### Correct Sentiment : 97\n",
    "\n",
    "-   Sentiment precision: 0.0662\n",
    "-   Sentiment recall: 0.4236\n",
    "-   Sentiment F: 0.1145\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluation for RU\n",
    "\n",
    "#### Entity in gold data: 389\n",
    "\n",
    "#### Entity in prediction: 1816\n",
    "\n",
    "#### Correct Entity : 266\n",
    "\n",
    "-   Entity precision: 0.1465\n",
    "-   Entity recall: 0.6838\n",
    "-   Entity F: 0.2413\n",
    "\n",
    "#### Correct Sentiment : 129\n",
    "\n",
    "-   Sentiment precision: 0.0710\n",
    "-   Sentiment recall: 0.3316\n",
    "-   Sentiment F: 0.1170\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
