{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions and set up \n",
    "\n",
    "## Take note that we take log for the transtion and emmmsion probablities to correct for underflow issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def readFile(filepath):\n",
    "    y_count = {}\n",
    "    emission_count = {}\n",
    "    transition_counts = {}\n",
    "    training_observations_x = set()\n",
    "\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        y_i = \"START\"\n",
    "        y_count[\"START\"] = 1\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                transition_counts[(y_i, \"STOP\")] = transition_counts.get((y_i, \"STOP\"), 0) + 1\n",
    "                y_count[\"STOP\"] = y_count.get(\"STOP\", 0) + 1\n",
    "                y_i = \"START\"\n",
    "                y_count[\"START\"] = y_count.get(\"START\", 0) + 1\n",
    "                continue\n",
    "\n",
    "            last_space_idx = line.rfind(\" \")\n",
    "            x, y_j = line[:last_space_idx], line[last_space_idx + 1:]\n",
    "            transition_counts[(y_i, y_j)] = transition_counts.get((y_i, y_j), 0) + 1\n",
    "            emission_count[(y_j, x)] = emission_count.get((y_j, x), 0) + 1\n",
    "            y_count[y_j] = y_count.get(y_j, 0) + 1\n",
    "            training_observations_x.add(x)\n",
    "            y_i = y_j\n",
    "        \n",
    "        if y_i != \"START\":\n",
    "            transition_counts[(y_i, \"STOP\")] = transition_counts.get((y_i, \"STOP\"), 0) + 1\n",
    "            y_count[\"STOP\"] = y_count.get(\"STOP\", 0) + 1\n",
    "        else: \n",
    "            y_count[\"START\"] -=1\n",
    "    return y_count, emission_count, transition_counts, training_observations_x\n",
    "\n",
    "def readDevIn(filePath):\n",
    "    x_sequences = []\n",
    "    current_sequence = []\n",
    "    with open(filePath, 'r') as file:\n",
    "        for line in file:\n",
    "            x = line.strip()\n",
    "            if not x:\n",
    "                if current_sequence: \n",
    "                    x_sequences.append(current_sequence)\n",
    "                    current_sequence = [] \n",
    "            else:\n",
    "                current_sequence.append(x)\n",
    "        if current_sequence:\n",
    "            x_sequences.append(current_sequence)\n",
    "    return x_sequences\n",
    "\n",
    "def write_seq_pairs_to_file(file_path, list_of_sequences):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for seq_pairs in list_of_sequences:\n",
    "            for x, y in seq_pairs:\n",
    "                 file.write(f\"{x} {y}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def emission_parameters_updated(x_t,y_t,y_count,emission_count,training_observations_x,k=1):\n",
    "    if(x_t in training_observations_x):\n",
    "        ## if count does not exist we shall set the return value to be 0 without dividing to prevent divide by 0 error \n",
    "        if(emission_count.get((y_t,x_t),0) == 0 ):\n",
    "            return float(\"-inf\")\n",
    "        ## we take log to avoid underflow issues \n",
    "        return math.log(emission_count.get((y_t,x_t),0)/(y_count[y_t]+k))\n",
    "    else:\n",
    "        ## we take log to avoid underflow issues \n",
    "        return math.log(k/(y_count[y_t]+k))\n",
    "\n",
    "def transition_parameters(y_i, y_j, transition_count, y_count):\n",
    "    numerator = transition_count.get((y_i, y_j), 0)\n",
    "    denominator = y_count[y_i]\n",
    "    if numerator == 0:    ## if count does not exist we shall set the return value to be 0 without dividing to prevent log zero err \n",
    "        return float('-inf')\n",
    "    return math.log(numerator / denominator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3 : Thought process of the modified viterbi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modification of the veterbi algorithm in each entry of the DP memo we store not just a singular best score at the position but a sorted list (decreasing order) of k best scores , each computed from a different transition. Also take note that betweeen each transtion from state u at layer j-1 to v at layer j , we must also consider the the trasition from anyone of the k scores stored in scores[(j-1,u)], since scores[(j-1,u)] contains k sets of scores , this is becasue each of those score are a result of a different path taken to arrive at node (j-1,u).  \n",
    "\n",
    "In addition to the just storing the k scores in the list at each memo slot scores[(j,v)] we take one futher step to store, backpointers to the parents as well the index of in which take path predesessor score from i.e scores[(j-1,u)] , this is done so as to ease the process of backtracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implemenatation of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_best_viterbi(y_count, emission_count, transition_counts, training_observations_x, x_input_seq, k=1):\n",
    "    \"\"\"\n",
    "        In this modification of the veterbi algorithm in each entry of the DP memo we store not just a singular best score at the position\n",
    "        but a sorted list (decreasing order) of k best scores , each computed from a different transition. Also take note that betweeen each \n",
    "        transtion from state u at layer j-1 to v at layer j , we must also consider the the trasition from anyone of the k scores stored in scores[(j-1,u)],\n",
    "        since scores[(j-1,u)] contains k sets of scores , this is becasue each of those score are a result of a different path taken to arrive at node (j-1,u). \n",
    "\n",
    "        In addition to the just storing the k scores in the list at each memo slot scores[(j,v)] we take one futher step to store, backpointers to the parents as well\n",
    "        the index of in which take path predesessor score from i.e scores[(j-1,u)] , this is done so as to ease the process of backtracking.\n",
    "    \"\"\"\n",
    "    n = len(x_input_seq)\n",
    "    states = list(y_count.keys())\n",
    "\n",
    "    scores = {}\n",
    "    for i in range(n+1):\n",
    "        for state in states: \n",
    "            score_list = []\n",
    "            for j in range(k):\n",
    "                score_list.append((float('-inf'), None , None))\n",
    "            scores[(i,state)] = score_list\n",
    "\n",
    "    scores[(n+1,\"STOP\")] = None \n",
    "    \n",
    "    # for i in range(k):\n",
    "    #     scores[(0, \"START\")][i] = [(0.0, None, None)]\n",
    "    scores[(0, \"START\")] = [(0.0, None, None)]\n",
    "\n",
    "    for t in range(1, n+1):\n",
    "        for v in states:\n",
    "            if (v == \"START\") or (v == \"STOP\"): continue \n",
    "            all_scores = [] # we maintain a list that stores all the transitions from u to v and store the respective score they result in  \n",
    "            for u in states:\n",
    "                if (u == \"STOP\") or (u == \"START\" and t != 1): continue   # this is simply done to clean up print statements in the debugging phase removing these does not change the result\n",
    "                for idx , score_tup in enumerate(scores[(t-1, u)]): \n",
    "                    # This extra for loop is the main point of divergence form the original viterbi algorithm as we take multiple transition\n",
    "                    # from each previous state u , as detailed above we must do this take into account the top k paths at the previosu node u \n",
    "                    emission_prob = emission_parameters_updated(x_input_seq[t-1], v, y_count, emission_count, training_observations_x, 1)\n",
    "                    transition_prob = transition_parameters(u, v, transition_counts, y_count)\n",
    "                    current_v_score = score_tup[0] + emission_prob + transition_prob\n",
    "                    all_scores.append((current_v_score, u , idx)) \n",
    "                        #print(\"layer {0} ({1},{2})->{3} cs:{4}\".format(t,u,idx,v,current_v_score))\n",
    "            #extract the k best scores and update the memo \n",
    "            scores[(t, v)] = sorted(all_scores, reverse=True)[:k] # we slice this list to consider only the top k scores , since we only need k paths at the terminal STOP node \n",
    "            #print(\"score list at ({0:>},{1:<}):{2}\".format(t,v,scores[(t, v)]))\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    # Final step to STOP\n",
    "    all_scores = []\n",
    "    for u in states:\n",
    "        if (u == \"START\") or (u == \"STOP\"): continue \n",
    "        #for idxInParentScoreList in range(k):\n",
    "        for idx , score_tup in enumerate(scores[(n, u)]): # Pelase refer to thte enumeration line above similar ligic applies here too (i mean refer to the main loop of virterbi)\n",
    "            transition_prob = transition_parameters(u, \"STOP\", transition_counts, y_count)\n",
    "            current_v_score = score_tup[0] + transition_prob\n",
    "            all_scores.append((current_v_score, u , idx))\n",
    "                #print(\"layer {0} ({1},{2})->{3} cs:{4}\".format(n+1,u,idx,\"STOP\",current_v_score))\n",
    "    scores[(n+1, \"STOP\")] = sorted(all_scores, reverse=True)[:k]\n",
    "    #print(\"score list at ({0:>},{1:<}):{2}\".format(n+1,\"STOP\",scores[(n+1, \"STOP\")])) \n",
    "\n",
    "    ### Now that we have arrived at the stop state , similar to original viterbi algorithm we have computes the best path from START to STOP \n",
    "    ### However instead of getting merely one best score we now have colelcted the top k \n",
    "    ### however we take up a extra compoutation to do this the TC of the forward parse of the algorithm now increases to O(n*kT^2*log(kT^2)*T^2) \n",
    "    # as we have kT^2 possible transtions scores and have to perform sorting on them which takes kT^2*log(kT^2)\n",
    "    ### All we have to do now is use backpropagation to build the k best paths from STOP all thte way back to START \n",
    "\n",
    "    # Reconstruct k-best paths\n",
    "    k_best_paths = []\n",
    "    score_list = [(n + 1, \"STOP\")]\n",
    "    for idx_in_STOP_list in range(k): ## For each of the scores at scores[(n+1,\"STOP\")] we backtrack all the way to the \"START\" \n",
    "        path = []\n",
    "        #print(\"printing final score list:\", scores[(n+1,\"STOP\")], idx_in_STOP_list)\n",
    "        score , parent , idx_in_parent = scores[(n+1,\"STOP\")][idx_in_STOP_list]\n",
    "        for i in range(n,0,-1):\n",
    "            #print(score,parent,idx_in_parent)\n",
    "            path.insert(0,parent)\n",
    "            score , parent , idx_in_parent = scores[(i,parent)][idx_in_parent]\n",
    "        #print(path)\n",
    "        k_best_paths.append(path)\n",
    "    #print(k_best_paths)\n",
    "    return k_best_paths , scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additonal things to note about modification:\n",
    "1) since we use baack pointers to both the parent and the index in the parents list we can perform backproagation for each of the paths in O(n) <br>\n",
    "2) since we have to do this for k differnt score the time complexity is a resultant O(kn) <br>\n",
    "3) Therefore the overall TC of decoding k best paths would be O(n*kT^2*log(kT^2)*T^2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to write output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelNwrite(readDevInPath, y_count, emission_count, transition_count, training_observations_x,writeFilePathList,k_paths_to_extract):\n",
    "    x_sequences = readDevIn(readDevInPath)\n",
    "    k = max(k_paths_to_extract)\n",
    "    list_of_sequences = {}\n",
    "    for paths in k_paths_to_extract:\n",
    "        list_of_sequences[paths] = []\n",
    "\n",
    "    for x_input_seq in x_sequences: \n",
    "        k_best_paths , scores = k_best_viterbi(y_count, emission_count, transition_count ,training_observations_x ,x_input_seq,k)\n",
    "        for paths in k_paths_to_extract:\n",
    "            list_of_sequences[paths].append(list(zip(x_input_seq,k_best_paths[paths-1])))\n",
    "    \n",
    "    for write_loc , path_number in list(zip(writeFilePathList,k_paths_to_extract)):\n",
    "        write_seq_pairs_to_file(write_loc,list_of_sequences[path_number])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_count_RU, emission_count_RU, transition_count_RU,training_observations_x_RU = readFile(\"./Data/RU/train\")\n",
    "buildModelNwrite(\"./Data/RU/dev.in\", y_count_RU, emission_count_RU, transition_count_RU, training_observations_x_RU,[\"./Data/RU/dev.p3.1st.out\",\"./Data/RU/dev.p3.2nd.out\",\"./Data/RU/dev.p3.8th.out\"],[1,2,8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RU results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RU 1st output\n",
    "\n",
    "Entity in gold data: 389  \n",
    "Entity in prediction: 490  \n",
    "\n",
    "Correct Entity: 189  \n",
    "Entity precision: 0.3857  \n",
    "Entity recall: 0.4859  \n",
    "Entity F: 0.4300  \n",
    "\n",
    "Correct Sentiment: 129  \n",
    "Sentiment precision: 0.2633  \n",
    "Sentiment recall: 0.3316  \n",
    "Sentiment F: 0.2935  \n",
    "\n",
    "---\n",
    "\n",
    "## RU 2nd output\n",
    "\n",
    "Entity in gold data: 389  \n",
    "Entity in prediction: 696  \n",
    "\n",
    "Correct Entity: 202  \n",
    "Entity precision: 0.2902  \n",
    "Entity recall: 0.5193  \n",
    "Entity F: 0.3724  \n",
    "\n",
    "Correct Sentiment: 124  \n",
    "Sentiment precision: 0.1782  \n",
    "Sentiment recall: 0.3188  \n",
    "Sentiment F: 0.2286  \n",
    "\n",
    "---\n",
    "\n",
    "## RU 8th output\n",
    "\n",
    "Entity in gold data: 389  \n",
    "Entity in prediction: 703  \n",
    "\n",
    "Correct Entity: 172  \n",
    "Entity precision: 0.2447  \n",
    "Entity recall: 0.4422  \n",
    "Entity F: 0.3150  \n",
    "\n",
    "Correct Sentiment: 94  \n",
    "Sentiment precision: 0.1337  \n",
    "Sentiment recall: 0.2416  \n",
    "Sentiment F: 0.1722\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_count_ES, emission_count_ES, transition_count_ES,training_observations_x_ES = readFile(\"./Data/ES/train\")\n",
    "buildModelNwrite(\"./Data/ES/dev.in\", y_count_ES, emission_count_ES, transition_count_ES,training_observations_x_ES ,[\"./Data/ES/dev.p3.1st.out\",\"./Data/ES/dev.p3.2nd.out\",\"./Data/ES/dev.p3.8th.out\"],[1,2,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES 1st Output\n",
    "Entity in gold data: 229 <br>\n",
    "Entity in prediction: 542<br>\n",
    "<br>\n",
    "Correct Entity: 134<br>\n",
    "Entity Precision: 0.2472<br>\n",
    "Entity Recall: 0.5852<br>\n",
    "Entity F: 0.3476<br>\n",
    "<br>\n",
    "Correct Sentiment: 97<br>\n",
    "Sentiment Precision: 0.1790<br>\n",
    "Sentiment Recall: 0.4236<br>\n",
    "Sentiment F: 0.2516<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ES 2nd Output\n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 436<br>\n",
    "<br>\n",
    "Correct Entity: 117<br>\n",
    "Entity Precision: 0.2683<br>\n",
    "Entity Recall: 0.5109<br>\n",
    "Entity F: 0.3519<br>\n",
    "<br>\n",
    "Correct Sentiment: 65<br>\n",
    "Sentiment Precision: 0.1491<br>\n",
    "Sentiment Recall: 0.2838<br>\n",
    "Sentiment F: 0.1955<br>\n",
    "\n",
    "---\n",
    "\n",
    "## ES 8th Output\n",
    "Entity in gold data: 229<br>\n",
    "Entity in prediction: 438<br>\n",
    "<br>\n",
    "Correct Entity: 102<br>\n",
    "Entity Precision: 0.2329<br>\n",
    "Entity Recall: 0.4454<br>\n",
    "Entity F: 0.3058<br>\n",
    "<br>\n",
    "Correct Sentiment: 56<br>\n",
    "Sentiment Precision: 0.1279<br>\n",
    "Sentiment Recall: 0.2445<br>\n",
    "Sentiment F: 0.1679<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
